<!DOCTYPE HTML><!-- _partial/head -->
<html>
<head>
  <meta charset="utf-8">
  
  <title>第 2 页 | 归档 | 风清阳明</title>
  <meta name="author" content="Jinwen">
  
  <meta name="description" content="互联网，机器学习，金融，生活">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="风清阳明"/>

  
    <meta property="og:image" content="undefined"/>
  

  
    <link rel="alternative" href="/true" title="风清阳明" type="application/atom+xml">
  
  
    <link href="/favicon.png" rel="icon">
  

  
  <link rel="stylesheet" href="/css/bootstrap.cerulean.min.css" media="screen" type="text/css">
  
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  <!-- _partial/post/google_analytics -->



</head>

 <body>
  <!-- _partial/navigation -->
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">风清阳明</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 <!-- archive -->
<!-- _partial/archive -->


<!-- archive.title -->
<div class="page-header">
  <h1 class="archive-title">归档</h1>
</div>

<div class="row page">
  <!-- cols -->
  
  <div class="col-md-9">
	

	  
	  <!-- display as entry -->
	  <div class="mypage">
	  
	  <!-- _partial/post/title -->
<div class="panel-heading">

	
	<!-- display as entry -->
		<h3 class="xtitle">
			<a href="/2014/07/30/socket_python/" >用Python编Socket入门程序</a>
			<span class="text-muted pull-right"><small> 7月 30 2014 </small></span>
		</h3>
	

</div>

	  <!-- _partial/post/entry -->
<div class="panel-body">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <p>本地进程通信有多种方法，那么网络中进程是如何通信的？一种流行的方法就是Socket编程，Socket可以看成是一个抽象“客栈”，所有网络中的进程带着自己的身份证号码（IP地址、端口号）来投宿，然后在客栈里面进行信息交流，至于你们这些人在里面聊些什么，放心，一般人是不清楚滴。</p>
<p>由于Python用简洁代码能实现丰富功能，所以这里就用TA了，不用纠结于语法细节问题。整个Socket编程分为服务器和客户端两方面编程,下面分开说明：</p>
<p>服务器端编程分为6步：</p>
<ol>
<li>建立一个Socket对象； </li>
<li>绑定到具体IP地址和端口号；</li>
<li>设置能连到本服务器的客户端数目；</li>
<li>检测是否有客服端的请求信息；</li>
<li>一旦和客户端建立好了连接，就可以处理这些信息了（发送和接收）；</li>
<li>关闭连接。</li>
</ol>
<p>客户端编程分为4步：</p>
<ol>
<li>建立Socket对象；</li>
<li>连接到服务器；</li>
<li>信息处理（发送和接收）；</li>
<li>关闭连接。</li>
</ol>
<p>好了，上代码（网上大致相同，具体参数可以去查阅）</p>
<p>服务器端：</p>
<pre><code>import <span class="keyword">socket</span>  
sock = <span class="keyword">socket</span>.<span class="keyword">socket</span>(<span class="keyword">socket</span>.AF_INET,<span class="keyword">socket</span>.SOCK_STREAM) <span class="comment">#建立对象</span>
sock.<span class="keyword">bind</span>((<span class="string">'localhost'</span>,<span class="number">8001</span>)) <span class="comment">#绑定具体IP地址和端口号</span>
sock.<span class="keyword">listen</span>(<span class="number">100</span>)  <span class="comment">#设置能连接的客户端数目</span>
<span class="keyword">while</span> True:
    connection,address = sock.<span class="keyword">accept</span>() <span class="comment">#监听是否有客户端请求</span>
    connection.settimeout(<span class="number">10</span>)
    buf = connection.<span class="keyword">recv</span>(<span class="number">100</span>) <span class="comment">#信息接收成功，返回接收的信息buf</span>
    <span class="keyword">if</span> buf == <span class="string">'hi'</span>: 
        connection.<span class="keyword">send</span>(<span class="string">'welcome to Socket!'</span>) <span class="comment">#发送信息</span>
    <span class="keyword">else</span>:
        connection.<span class="keyword">send</span>(<span class="string">'...'</span>)
    connection.<span class="keyword">close</span>() <span class="comment">#关闭连接</span>
</code></pre><p>客户端：</p>
<pre><code>import <span class="keyword">socket</span>
sock = <span class="keyword">socket</span>.<span class="keyword">socket</span>(<span class="keyword">socket</span>.AF_INET,<span class="keyword">socket</span>.SOCK_STREAM) <span class="comment">#建立对象</span>
sock.<span class="keyword">connect</span>((<span class="string">'localhost'</span>,<span class="number">8001</span>)) <span class="comment">#连接到服务器绑定的位置（地址：端口）</span>
sock.<span class="keyword">send</span>(<span class="string">'hi'</span>)  <span class="comment">#打个招呼，发送一个信息</span>
<span class="keyword">print</span> sock.<span class="keyword">recv</span>(<span class="number">1024</span>) <span class="comment">#打印接收到的信息</span>
sock.<span class="keyword">close</span>() <span class="comment">#关闭连接</span>
</code></pre><p>运行：<br>先在命令行模式下运行服务器程序，然后再运行客户端程序，就会打印出”welcome to Socket!”，简单的运行模式就成功了。</p>

	
	</div>
	

</div>
	<a type="button" href="/2014/07/30/socket_python/#more" class="btn btn-default more">阅读此文</a>
</div>

	  
	  <!-- _partial/post/title -->
<div class="panel-heading">

	
	<!-- display as entry -->
		<h3 class="xtitle">
			<a href="/2014/07/29/cf/" >协同过滤</a>
			<span class="text-muted pull-right"><small> 7月 29 2014 </small></span>
		</h3>
	

</div>

	  <!-- _partial/post/entry -->
<div class="panel-body">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <p>网上买东西时，经常看到一些推荐商品列表，那么这些列表是如何得到的？推荐算法的结果。在Web 2.0时代，每个人都能参与页面内容的编写，贡献自己的数据，这就出现了一个新词：集体智慧。集体智慧编程就是利用每一个个体的数据和行为来建模。而这些商品推荐模型中的协同过滤算法是集体智慧编程中的一个重要方面。</p>
<h1 id="协同过滤">协同过滤</h1>
<p>俗话说看一个人的品味，就看TA的圈子。看书、看电影、玩游戏等等，如果自己不知道该看什么书，哪些电影精彩，什么游戏比较有意思，这个时候就需要咨询一些“志同道合”的人做个介绍，听听他们的评论，然后再加以选择。 这么简简单单的几句话，其实里面涉及到非常重要的三个方面：  </p>
<ol>
<li>需要弄清楚咨询对象的喜好；</li>
<li>TA的喜好是否和自己相似；</li>
<li>根据他们喜欢的书、电影、游戏，选择一些自己没有接触过的。</li>
</ol>
<p>只有完成这三个步骤，推荐才算基本完成，而实际设计中，这三步中的每一步都得小心谨慎，当然，第3步很简单。</p>
<p>协同过滤(CF)就利用了这样一种思想，也有类似的三步：</p>
<ol>
<li>设计特征向量，根据用户平时的一些行为（比如：评分、购买、查看时间等等）加权得到一个对各种物品的喜爱程度值，将各种物品的值组成一个特征向量；</li>
<li>计算特征向量的相似程度，所有用户对物体的特征向量组合成一个特征矩阵，在这里默认为行标记为用户，列标记为物品，所以，行向量之间的相似度反映得是用户之间的相似程度，列向量则反映物品之间的相似程度，因此衍生出两种模型，后面会介绍；</li>
<li>推荐排名，根据向量之间的相似度，得到一个排名，根据排名选择自己没有选的物品。</li>
</ol>
<h1 id="算法过程">算法过程</h1>
<h2 id="设计特征向量">设计特征向量</h2>
<p>如果用户对某件物品很喜欢，有很多方面会反应出来，TA会给一个评价很高的分数（当然实际中很少人会去打分）、或许会买下这个物品、也或许浏览了很久，根据这些收集到的数据，我们就可以采取一个加权得到一个总分数，来衡量该用户对物品的喜爱程度。当然实际中，需要降噪和归一化处理，之后才加权。</p>
<ol>
<li>降噪：就像前面说的，一个人在某个页面停留了很久，并不表示TA一直在看，可能离开电脑去做其他事情了，这时候，这个停留时间不一定反应喜爱程度，所以，实际中需要对收集的数据做一个降噪处理；</li>
<li>归一化：停留时间（可能几百秒）、评分（可能就是1到5的范围）等数据的幅度是不一样的，直接加权肯定是不行的，这个很好理解。</li>
</ol>
<p>经过前面这两步的处理，就可以加权得到一个喜爱程度值了，比如：买了某个物体，说明很喜欢，权值可以设得稍微大点。所有物体的喜爱程度值，就成为了我们的特征向量。</p>
<h2 id="计算相似度">计算相似度</h2>
<p>首先说明一下，计算向量的相似度有很多标准，这个根据实际情况加以选取，比方常见的欧几里得距离、Cosine距离、闵式距离、汉密尔顿距离等等，距离越大，相似度越低。</p>
<p>其次，前面说过，很多用户的特征向量组合到一起，形成了一个特征矩阵（行表示用户、列表示物品），如果计算行向量的相似度，则是用户之间的相似度，这种模型是基于用户的推荐算法（User CF)，如果是列向量，则这种模型是基于物品的推荐算法（Item CF）。</p>
<h2 id="推荐排名">推荐排名</h2>
<p>根据前面计算的相似度，得到一个排名，如何从这个排名里面选择呢？有两种方式：</p>
<ol>
<li>基于固定数量的选取，即自己必须从这个排名里面选择排在前K的相似向量；</li>
<li>基于固定距离的选取，即自己只从那些距离小于固定值a的向量。</li>
</ol>
<p>这两种方式也是根据实际情况来选取。</p>
<p>User CF的模型得到是一些行向量，对这些行向量做一个逻辑“或”操作，得到的向量就是推荐物体（当然需要去掉自己已经选择了物体）。</p>
<p>Item CF的模型得到的就是一些列向量，这些列向量各自代表的物体就是推荐的物体（同样需要去掉已经选择了的物体）。</p>
<p>到这里为止，这个协同过滤的基本过程就介绍完了。</p>
<h1 id="Apache_Mahout">Apache Mahout</h1>
<p>Mahout是一个不错的实现了协同过滤模型的开源软件，最好在Linux系统下操作，以后如果熟悉了这个软件，再详细介绍其相关使用。</p>
<p>reference:</p>
<p><a href="http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/" target="_blank">探索推荐引擎内部的秘密:协同过滤</a></p>
<p><a href="http://en.wikipedia.org/wiki/Collaborative_filtering" target="_blank">协同过滤</a> </p>

	
	</div>
	

</div>
	<a type="button" href="/2014/07/29/cf/#more" class="btn btn-default more">阅读此文</a>
</div>

	  
	  <!-- _partial/post/title -->
<div class="panel-heading">

	
	<!-- display as entry -->
		<h3 class="xtitle">
			<a href="/2014/06/19/k-means/" >k-means</a>
			<span class="text-muted pull-right"><small> 6月 19 2014 </small></span>
		</h3>
	

</div>

	  <!-- _partial/post/entry -->
<div class="panel-body">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <p>前面提到的都是监督学习，就是说训练样本的类别标签是已知的。然而实际中，大量的数据类别标签是未知的，所以，利用这些数据的学习算法（即非监督学习）显得举足轻重。聚类是一种非监督学习算法，而k-means算法是聚类算法中的代表。</p>
<h1 id="介绍">介绍</h1>
<p>k-means算法中的k表示聚成k类，整个聚类过程可以这么设想，在一大群人中间需要选出k个领头人（质心点），然后剩下所有人都开始追随离自己最近（距离）的领头人，同一个领头人带领的人归为一类，所以总共有k类。算法流程如下：    </p>
<ol>
<li>随机选出k个质心点；  </li>
<li>计算每个点与质心点的距离，将每个点和离它最近的质心点归为同一类；</li>
<li>在每一类中重新计算质心点；  </li>
<li>如果质心点发生变化，回到第2步；  </li>
<li>算法终止，得到k个类，每个类的质心点与同一类的点距离最小。</li>
</ol>
<p>整个思想都比较简单，但是由于k-means会陷入局部最小值，所以需要采取一些改进措施，后面会加以介绍。此外，对于距离的衡量，可以有多重方式，比如欧几里得距离、编辑距离等等。 </p>
<h1 id="代码">代码</h1>
<p>随机选取质心点的代码：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span>
    n = shape(dataSet)[<span class="number">1</span>]
    centroids = mat(zeros((k,n)))  <span class="comment"># 初始化质心点矩阵</span>
    <span class="comment">#质心点必须保证在所有数据点范围以内</span>
    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):
        minJ = min(dataSet[:,j]) 
        rangeJ = float(max(dataSet[:,j]) - minJ)
        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,<span class="number">1</span>))
    <span class="keyword">return</span> centroids
</code></pre><p>最原始的k-means算法代码：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span>
    m = shape(dataSet)[<span class="number">0</span>]   
    clusterAssment = mat(zeros((m,<span class="number">2</span>))) <span class="comment">#第一列存储类别好，第二列存储与质心点的距离  </span>
    centroids = createCent(dataSet, k)   <span class="comment">#初始化质心点</span>
    clusterChanged = <span class="keyword">True</span>   
    <span class="keyword">while</span> clusterChanged:
        clusterChanged = <span class="keyword">False</span>
        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):<span class="comment">#找到距离最近的质心点，并设置为同一类</span>
            minDist = inf; minIndex = -<span class="number">1</span>
            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):
                distJI = distMeas(centroids[j,:],dataSet[i,:])
                <span class="keyword">if</span> distJI &lt; minDist:
                    minDist = distJI; minIndex = j
            <span class="keyword">if</span> clusterAssment[i,<span class="number">0</span>] != minIndex: clusterChanged = <span class="keyword">True</span>
            clusterAssment[i,:] = minIndex,minDist**<span class="number">2</span>  <span class="comment">#更新当前数据点的类别</span>
        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):<span class="comment">#重新计算质心点</span>
            ptsInClust = dataSet[nonzero(clusterAssment[:,<span class="number">0</span>].A==cent)[<span class="number">0</span>]]
            centroids[cent,:] = mean(ptsInClust, axis=<span class="number">0</span>) <span class="comment">#将所有同一类点的平均值作为质心点</span>
    <span class="keyword">return</span> centroids, clusterAssment
</code></pre><p>前面已经说过，原始的k-means算法很容易陷入局部最小值（这个与k个质心点的初始化有关系）。一个衡量聚类好坏的是利用误差平方和（SUm of Squared Error, SSE），即当前类所有点与质心点的距离之和，在上段代码中clusterAssment的第二列记录了点与质心点之间的距离。</p>
<p>为了克服局部最小值问题，可以采用二分k-means算法，这个算法是先将所有的点作为一个类，然后选择每一个类进行二分，即k=2，选择划分后SSE之和最小的那个类进行划分，直到划分的数目等于k。</p>
<p>参考<a href="http://www.manning.com/pharrington/" target="_blank">《机器学习实战》</a>。</p>

	
	</div>
	

</div>
	<a type="button" href="/2014/06/19/k-means/#more" class="btn btn-default more">阅读此文</a>
</div>

	  
	  <!-- _partial/post/title -->
<div class="panel-heading">

	
	<!-- display as entry -->
		<h3 class="xtitle">
			<a href="/2014/06/08/adaboost/" >AdaBoost</a>
			<span class="text-muted pull-right"><small> 6月 8 2014 </small></span>
		</h3>
	

</div>

	  <!-- _partial/post/entry -->
<div class="panel-body">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <p>组合分类器模型是一种将弱分类器组合成强分类器的方法。弱分类器仅仅比随机猜测略好，如果存在多个这种弱分类器，就可以将其提升为强分类器，这就是弱分类器和强分类器的等价问题。Adaboost是组合分类器的代表，本文介绍Adaboost的原理。</p>
<h1 id="介绍">介绍</h1>
<p>先图示怎样将弱分类器组合成强分类器，初始数据线性不可分：<br><img src="http://d1zjcuqflbd5k.cloudfront.net/files/acc_263367/mQCF?response-content-disposition=inline;%20filename=1.jpg;&amp;Expires=1402194998&amp;Signature=BEME~LEgJ6X5R6W-TBi8T08pSuyXOre~6~bjlcjzzODjFKPMMLlyd~aOE0wfkSlU9lEPhjgki8h1QjVPbCSG1FkxP-dUrdqFe98rpYnvUodShBmDpNdyvgSmfvZi2wykW9GyJ8DG~W4bEczgNw15RFC94GggBS~pYDEFlSRN2hE_&amp;Key-Pair-Id=APKAJTEIOJM3LSMN33SA" alt=""></p>
<p>上面3个都是弱分类器（比随机猜测略好），通过加权组合它们，生成下面的强分类器。</p>
<p>Adaboost算法需要多次迭代，每个弱分类器会有一个权重来衡量最终分类所起的作用。 $\alpha_ih_i(x)$就是强分类器$H(x)$中弱分类器$h_i(x)$ 所占部分。同样，在训练弱分类器的过程之中，对于那些被分错的样本，需要着重考虑，下一次的迭代需要偏向将它们正确分类，所以，每一次迭代需要更改每一个样本的分布 $W_i$，即样本权重。</p>
<h1 id="AdaBoost算法">AdaBoost算法</h1>
<ol>
<li>初始化样本权重系数$w^{(1)}_i = 1/N, i = 1,\ldots,N$。（一共有N个样本，所以均匀权重）；</li>
<li>迭代M次，在第m次迭代中,通过最小化误差函数得到弱分类器$y_m(x)$，误差函数为$J_m = \sum_i^Nw^{(m)}_nI(y_m(X_n) \neq t_n)$,其中$I(x)$为指示函数。</li>
<li>估计当前弱分类器的质量：<br>$\epsilon_m = \frac{\sum_i^Nw^{(m)}_nI(y_m(X_n) \neq t_n)}{\sum_i^Nw^{(m)}_n}$<br>进一步得到当前分类器的权重：<br>$\alpha_m = \ln\frac{1-\epsilon_m}{\epsilon}$</li>
<li>更新样本的权重系数（保证分错的样本在下一次迭代权重要大)<br>$w^{(m+1)}_i =w^{(m)}_i e^{\alpha_mI(y_m(x_n) \neq t_n)}, i = 1,\ldots,N$<br>回到第2步，直到生成M个弱分类器；</li>
<li>加权组合所有的弱分类器得到最终的强分类器：<br>$H(x) = sign(\sum_m^M\alpha_my_m(x))$</li>
</ol>
<h1 id="实例代码">实例代码</h1>
<p>单一特征分类 </p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">stumpClassify</span><span class="params">(dataMatrix,dimen,threshVal,threshIneq)</span>:</span>
    <span class="comment">#一个弱分类器的结果，所有大于（或小于）阈值的数据标记为同一类</span>
    retArray = ones((shape(dataMatrix)[<span class="number">0</span>],<span class="number">1</span>))
    <span class="keyword">if</span> threshIneq == <span class="string">'lt'</span>:
        retArray[dataMatrix[:,dimen] &lt;= threshVal] = -<span class="number">1.0</span>
    <span class="keyword">else</span>:
        retArray[dataMatrix[:,dimen] &gt; threshVal] = -<span class="number">1.0</span>
    <span class="keyword">return</span> retArray
</code></pre><p>建立一个弱分类器</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">buildStump</span><span class="params">(dataArr,classLabels,D)</span>:</span>
    <span class="comment"># 数据矩阵和类标记向量</span>
    dataMatrix = mat(dataArr); labelMat = mat(classLabels).T
    m,n = shape(dataMatrix)  <span class="comment"># m,n分别代表训练实例的数量和特征维度</span>
    numSteps = <span class="number">10.0</span>; bestStump = {}; bestClasEst = mat(zeros((m,<span class="number">1</span>)))
    minError = inf <span class="comment">#初始化最小的错误率为无穷大</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):<span class="comment">#循环遍历所有特征维度，找出错误率最小的特征进行划分</span>
        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max();  <span class="comment"># 当前特征维度最大值最小值确定划分的范围</span>
        stepSize = (rangeMax-rangeMin)/numSteps  <span class="comment">#当前特征维度划分步长</span>
        <span class="keyword">for</span> j <span class="keyword">in</span> range(-<span class="number">1</span>,int(numSteps)+<span class="number">1</span>): <span class="comment">#遍历每个小块，确定划分的最优位置</span>
            <span class="keyword">for</span> inequal <span class="keyword">in</span> [<span class="string">'lt'</span>, <span class="string">'gt'</span>]: <span class="comment">#确定分界线两边的类别</span>
                threshVal = (rangeMin + float(j) * stepSize)
                predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal) <span class="comment"># 当前分类结果</span>
                errArr = mat(ones((m,<span class="number">1</span>)))
                errArr[predictedVals == labelMat] = <span class="number">0</span>  <span class="comment"># 当前错误率</span>
                weightedError = D.T*errArr  <span class="comment"># 加权样本分布权重的错误率</span>
                <span class="keyword">if</span> weightedError &lt; minError: <span class="comment">#找到错误率最小的划分块</span>
                    minError = weightedError
                    bestClasEst = predictedVals.copy()
                    bestStump[<span class="string">'dim'</span>] = i
                    bestStump[<span class="string">'thresh'</span>] = threshVal
                    bestStump[<span class="string">'ineq'</span>] = inequal
    <span class="keyword">return</span> bestStump,minError,bestClasEst
</code></pre><p>合成强分类器</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">adaBoostTrainDS</span><span class="params">(dataArr,classLabels,numIt=<span class="number">40</span>)</span>:</span>
    weakClassArr = []
    m = shape(dataArr)[<span class="number">0</span>]
    D = mat(ones((m,<span class="number">1</span>))/m)   <span class="comment">#均匀初始化样本的分布</span>
    aggClassEst = mat(zeros((m,<span class="number">1</span>)))
    <span class="keyword">for</span> i <span class="keyword">in</span> range(numIt):
        bestStump,error,classEst = buildStump(dataArr,classLabels,D)  <span class="comment"># 建立一个弱分类器</span>
        alpha = float(<span class="number">0.5</span>*log((<span class="number">1.0</span>-error)/max(error,<span class="number">1e-16</span>))) <span class="comment">#计算当前分类器的权重</span>
        bestStump[<span class="string">'alpha'</span>] = alpha  
        weakClassArr.append(bestStump)                  
        expon = multiply(-<span class="number">1</span>*alpha*mat(classLabels).T,classEst) 
        D = multiply(D,exp(expon))                              
        D = D/D.sum() <span class="comment">#更新样本分布（注意归一化）</span>
        aggClassEst += alpha*classEst 
        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,<span class="number">1</span>))) 
        errorRate = aggErrors.sum()/m  <span class="comment"># 计算错误率，直到错误率为0终止</span>
        <span class="keyword">if</span> errorRate == <span class="number">0.0</span>: <span class="keyword">break</span>
    <span class="keyword">return</span> weakClassArr,aggClassEst
</code></pre><p>测试</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">adaClassify</span><span class="params">(datToClass,classifierArr)</span>:</span>
    dataMatrix = mat(datToClass)
    m = shape(dataMatrix)[<span class="number">0</span>]
    aggClassEst = mat(zeros((m,<span class="number">1</span>)))
    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classifierArr)):
        classEst = stumpClassify(dataMatrix,classifierArr[i][<span class="string">'dim'</span>],\
                                 classifierArr[i][<span class="string">'thresh'</span>],\
                                 classifierArr[i][<span class="string">'ineq'</span>])
        aggClassEst += classifierArr[i][<span class="string">'alpha'</span>]*classEst  <span class="comment">#累积所有弱分类器的结果</span>
    <span class="keyword">return</span> sign(aggClassEst)
</code></pre><p>实例代码来自<a href="http://www.manning.com/pharrington/" target="_blank">《机器学习实战》</a>。</p>

	
	</div>
	

</div>
	<a type="button" href="/2014/06/08/adaboost/#more" class="btn btn-default more">阅读此文</a>
</div>

	  
	  <!-- _partial/post/title -->
<div class="panel-heading">

	
	<!-- display as entry -->
		<h3 class="xtitle">
			<a href="/2014/06/01/svm/" >支持向量机SVM</a>
			<span class="text-muted pull-right"><small> 6月 1 2014 </small></span>
		</h3>
	

</div>

	  <!-- _partial/post/entry -->
<div class="panel-body">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <p>支持向量机(Support Vector Machine，SVM)是最重要的分类器之一（还是加上“之一”两个字，这样保证不会错），下面开始简单推导出SVM。</p>
<p>SVM的别名是最大间隔分类器，这里的最大间隔指的是训练样本点到分隔超平面的间隔最大。在感知器中，没有考虑最大分隔这一条件，所以对于线性可分的数据集，会存在多个满足条件的分隔超平面，而在SVM中，加上“最大间隔这一条件”，使得分隔超平面唯一存在。</p>
<p>而在介绍SVM之前，需要了解拉格朗日乘子法的原理。</p>
<h1 id="拉格朗日乘子法">拉格朗日乘子法</h1>
<p>拉格朗日乘子法是凸优化中一个基础知识点，在高等数学中也接触过这一原理，但是在那里约束条件一般只是涉及到等式约束，其实在等式约束和不等式约束共存的前提下，整个过程的原理也是一样的。</p>
<h2 id="原始问题">原始问题</h2>
<p>$$ \min_{x \in R^n} f(x) \\<br>   s.t. c_i(x) \leq 0,i=1,2,\ldots,k\\<br>   \qquad h_j(x) = 0,j = 1,2,\ldots,l $$</p>
<p>其中，$f(x),c_i(x),h_j(x)$是定义在$R^n$上的连续可微函数。</p>
<p>解决这个优化问题可以采用拉格朗日乘子法，先引入拉格朗日函数：<br>$$L(x,\alpha,\beta) = f(x) + \alpha c(x) + \beta h(x)$$<br>其中，$\alpha,\beta$是k,l维的行向量，即拉格朗日乘子，且$\alpha \geq 0$。</p>
<p>考虑一下关于$x$的函数:<br>$$\theta (x) = \max_{\alpha,\beta;\alpha_i \geq 0} L(x,\alpha,\beta)$$</p>
<p>可以证明当x满足原始问题的约束条件时，$\theta (x) = f(x) $。（当约束条件的等号都成立时，$\theta (x)取得最大值$，反之，如果约束条件不满足，则\theta (x) 的取值趋向于无穷大）</p>
<p>此时，原始问题的极小化问题变为:</p>
<p>$$<br>\min_x \theta(x)<br>$$</p>
<h2 id="拉格朗日乘子法图解">拉格朗日乘子法图解</h2>
<p>想象一下目标函数关于变量的等高线，以二维变量为例，如下图所示（来源于维基百科）:<br><img src="https://s3-us-west-2.amazonaws.com/droplr.storage/files/acc_263367/CsFD?AWSAccessKeyId=AKIAJSVQN3Z4K7MT5U2A&amp;Expires=1401629870&amp;Signature=OjYyMqzDKIj%2FylGgFXAUuUegUzs%3D" alt=""></p>
<p>每条等高线对应着同一个目标函数值。约束条件的函数可以看成一条曲线，与相应等高线相交的点就是满足约束条件的目标值和相应的变量值。为了找到最优的目标值，需要使等高线与约束曲线相切。在相切点目标函数的梯度和约束条件函数的梯度是共线的，即$f(x)$的梯度等于 $\alpha h(x)$的梯度(假设只有等式约束)。</p>
<h2 id="对偶问题">对偶问题</h2>
<p>设函数$$D(\alpha,\beta) = min_{x} L(x,\alpha,\beta)$$</p>
<p>再极大化这一函数，可得原问题的对偶问题：<br>$$ \max_{\alpha,\beta;\alpha_i \geq 0} D(\alpha,\beta)\\<br>   s.t. \alpha_i \geq 0, i = 1,\ldots,k<br>$$</p>
<p>只要满足KKT条件，对偶问题的最优解和原始问题的最优解是一致的。在这里KKT条件就是</p>
<ol>
<li>$L(x,\alpha,\beta)$对x求导为0；</li>
<li>$h(x) = 0$; </li>
<li>$\alpha g(x) = 0$</li>
</ol>
<p>整个拉格朗日乘子法是解决带有等式和不等式约束条件的优化方法，特别是其将原问题变为对偶问题（最优解一致）能够很方便得对模型进行扩展，比如后面将要提到的核方法。</p>
<h1 id="SVM">SVM</h1>
<p>设分离超平面为:<br>$$w^Tx + b = 0$$</p>
<p>决策函数为:<br>$$f(x) = sign(w^Tx+b)$$</p>
<p>数据点到分离面的距离为$|w^T+b|$，当$w^T+b$的符号与标记$y$一致时表示分类正确，这样用$y(w^Tx+b)$来表示分类的正确性和确信度，符号为正表示分类正确，值的大小表示到分隔面的距离。</p>
<h2 id="函数间隔和几何间隔">函数间隔和几何间隔</h2>
<p>样本点的函数间隔为:<br>$$\hat \gamma_i = y_i(w^T x_i + b)$$</p>
<p>训练集的函数间隔就是:<br>$$\hat \gamma = \min_{i=1,\ldots,N} \hat \gamma_i$$</p>
<p>由于w,b成比例变化时，超平面不变，但是函数间隔变了，这样通过函数间隔无法刻画超平面的变化，需要一个其他的间隔来进行衡量，由此引出几何间隔。</p>
<p>样本点的几何间隔为：<br>$$Y_i = y_i(\frac{w^T}{||w^T||} x_i + \frac{b}{||w^T||})$$</p>
<p>训练集的间隔为：<br>$$Y = \min_{i=1,\ldots,N} Y_i$$</p>
<p>w,b成比例变化时，几何间隔不变，对应着超平面不变，所以可以通过w,b结合几何间隔来刻画超平面的位置。</p>
<p>函数间隔和几何间隔的关系很明显为$Y_i = \frac{\hat \gamma_i}{||W||},Y = \frac{\hat \gamma}{||W||}$。</p>
<h2 id="间隔最大化">间隔最大化</h2>
<p>现在的目的就是最大化训练集的几何间隔，所以优化函数如下：<br>$$\max_{w,b} Y \\<br>  s.t.\qquad y_i((\frac{w^T}{||w^T||} x_i + \frac{b}{||w^T||}) \geq Y,i=1,\dots,N$$</p>
<p>其中约束条件表示任何数据点的几何间隔至少是$Y$，注意那些离分隔面最近的点的距离就是$Y$，这些点被称作为支持向量，这也是支持向量机名称的来源。分隔面只由这些支持向量决定，训练集中其他数据点的变化对平面没有什么影响。</p>
<p>利用几何间隔和函数间隔的关系，用函数间隔来表示这一优化函数，如下：<br>$$\max_{w,b} \frac{\hat \gamma}{||w||} \\<br>  s.t.\qquad y_i((w^Tx_i + b) \geq \hat \gamma,i=1,\dots,N$$</p>
<p>前面说过函数间隔对于分隔面的位置没有影响，所以为了方便，令$\hat \gamma = 1$，同时为了规范化优化函数（一般是考虑最小化问题），可以将最大化$\frac{1}{||w||}$转化成最小化$\frac{1}{2}||w||^2$。于是SVM的线性可分优化问题最终确定为如下：<br>$$\min_{w,b}\frac{1}{2}||w||^2\\<br>s.t.\qquad y_i(w^Tx_i+b) - 1 \geq 0, i = 1,2,\ldots,N$$</p>
<p>求解这一优化问题的最优解，即是最优分离超平面，支持向量就是满足$y_i(w^Tx_i+b) - 1 = 0$的点。</p>
<h2 id="对偶求解">对偶求解</h2>
<p>求解前面的优化问题要利用到前面提到的朗格朗日乘子法，且需要转化成对偶问题。转化成对偶问题有两个好处：</p>
<ol>
<li>求解方便；</li>
<li>引入核函数，将SVM推广到非线性分类上去。</li>
</ol>
<p>根据前面介绍的对偶关系，对偶问题是一个极大极小值问题：<br>$$\max<em>{\alpha} \min</em>{w,b} L(w,b,\alpha)$$</p>
<p>所以需要先求$L(w,b,\alpha)$的极小值，再求对$\alpha$的极大。 </p>
<p>第一步：求解$\min_{w,b} L(w,b,\alpha)$ </p>
<p>分别对$w,b$求导并令其等于0, 然后代入拉格朗日函数得到</p>
<p>$$\min_{w,b} L(w,b,\alpha) = $$</p>
<p>$$-\frac{1}{2}\sum_{i=1}^N \sum_j^N \alpha_i\alpha_jy_iy_j(x_ix_j) +\sum_i^N\alpha_i$$</p>
<p>第二步：求$\min_{w,b} L(w,b,\alpha)$的极大，即对偶问题:</p>
<p>$$\max -\frac{1}{2}\sum_{i=1}^N \sum_j^N \alpha_i\alpha_jy_iy_j(x_ix_j) +\sum_i^N\alpha_i$$</p>
<p>$$s.t. \qquad \sum_{i=1}^N \alpha_iy_i = 0$$</p>
<p>$$\qquad \alpha_i \geq 0,i = 1,2,\ldots,N$$</p>
<h2 id="线性不可分问题">线性不可分问题</h2>
<p>前面是在线性可分的假设基础之上，如果数据不是线性可分的，这个时候需要引入松弛变量$\xi$，因为不管怎么分，都会有些数据是分错的，唯一能够做的就是使分错的数目最小（或者代价最小）。所以，优化函数变成了以下形式：<br>$$\min_{w,b,\xi} \frac{1}{2} ||w||^2+C\sum_i^N\xi_i$$</p>
<p>$$s.t. y_i(wx_i+b) \geq 1 - \xi_i,i=1,2,\ldots,N$$</p>
<p>$$\xi_i \geq 0,i=1,2,\ldots,N$$</p>
<h2 id="非线性支持向量机">非线性支持向量机</h2>
<p>有时候，数据在当前维度下不是线性可分的，但是将其映射到高维空间之后，数据变得线性可分。这个观念也可以从另一方面理解，如果数据线性不可分，但是可以画一条曲线将它们不同的类分离，而这条曲线可以理解成高维空间的一条直线，说明这些数据在高维空间是线性可分的。比方说$x^2+y^2 = 1$在平面空间内是一个圆，而在空间$(z_1=x^2,z_2 = y^2)$空间内就是一条$z_1+z_2 = 1$的直线。</p>
<p>解决这种问题就需要利用到核函数的概念。</p>
<h3 id="核函数">核函数</h3>
<p>低维到高维的变换需要一个映射函数：<br>$$\phi: X \rightarrow H$$</p>
<p>而核函数表示映射过到高维空间中数据点的内积，即函数$K(x,y)$满足：<br>$$K(x,y) = \phi(x)\cdot \phi(y), x,y \in X$$</p>
<p>其中$\phi(x)\cdot \phi(y)$表示内积。</p>
<p>如何利用核函数解决高维线性可分问题？</p>
<p>还记得前面提到的对偶问题的优势2吗？在对偶问题的目标函数中，存在$x_ix_j$这一因子，如果将其替换为$\phi(x_i)\cdot \phi(x_j)$，就可以直接应用核函数来将数据映射到高维空间进行线性划分。</p>
<p>核函数最直接的优势就是我们不需要去寻找映射函数$\phi$，而关心数据本身在高维空间的分布。</p>
<p>哪些函数能够成为核函数呢？需要满足正定核函数的条件。其实，平时利用的最多多的也就是那几个：高斯核、径向基核等等，如果需要实际体验，可以尝试libsvm这个软件，非常好用，有matlab接口(^_^)。</p>
<p>至此，整个SVM的初步流程理清了一遍，大致过程应该可以明了。</p>
<p>参考文献：  </p>
<ol>
<li>《统计机器学习》，李航  </li>
<li>《机器学习实战》，Peter Harrington</li>
</ol>

	
	</div>
	

</div>
	<a type="button" href="/2014/06/01/svm/#more" class="btn btn-default more">阅读此文</a>
</div>

	  
	  </div>
	  <div>
	  <center>
	  <!-- _partial/index_pagination -->
<div class="pagination">
<ul class="pagination">
	 
		
    	<li class="prev"><a href="/archives/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i> 上一页</a></li>
  		

        <li><a href="/"><i class="fa fa-home"></i>Home</a></li>

		
		   <li class="next"> <a href="/archives/page/3/" class="alignright next">下一页<i class="fa fa-arrow-circle-o-right"></i></a> </li>
        
	
</ul>
</div>

	  </center>
	  </div>

	  

</div> <!-- col-md-9/col-md-12 -->


<!-- _partial/sidebar -->
<div class="col-md-3">
	<div id="sidebar">
	
			<!-- _widget/search -->
<div class="form-group has-success has-feedback">
  <form action="//google.com/search" method="get" accept-charset="utf-8" >
    <input type="search" name="q" results="0" placeholder="搜索" class="form-control">
    <input type="hidden" name="q" value="site:xiaojinwen.com">
  </form>
</div>

		
			<!-- _widget/category -->

	<div class="widget">
		<h4>分类</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/Python/">Python<span>1</span></a></li>
		
			<li><a href="/categories/人工智能/">人工智能<span>10</span></a></li>
		
			<li><a href="/categories/工作/">工作<span>1</span></a></li>
		
			<li><a href="/categories/机器学习/">机器学习<span>1</span></a></li>
		
			<li><a href="/categories/生活/">生活<span>1</span></a></li>
		
			<li><a href="/categories/读书/">读书<span>1</span></a></li>
		
		</ul>
	</div>


		
			<!-- _widget/tagcloud -->

	<div class="widget">
		<h4>标签云</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/tags/求职/">求职<span>1</span></a></li>
		
			<li><a href="/tags/HMM/">HMM<span>3</span></a></li>
		
			<li><a href="/tags/机器学习实战/">机器学习实战<span>7</span></a></li>
		
			<li><a href="/tags/说明/">说明<span>1</span></a></li>
		
			<li><a href="/tags/Socket/">Socket<span>1</span></a></li>
		
			<li><a href="/tags/人工智能/">人工智能<span>1</span></a></li>
		
			<li><a href="/tags/王小波/">王小波<span>1</span></a></li>
		
		
		</ul>
	</div>


		
			<!-- _widget/recent_posts -->

<div class="widget">
  <h4>最新文章</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2014/12/14/job_finding/" ><i class="fa fa-file-o"></i>求职小结</a>
      </li>
    
      <li>
        <a href="/2014/11/13/silent_majority/" ><i class="fa fa-file-o"></i>沉默的大多数</a>
      </li>
    
      <li>
        <a href="/2014/10/07/hmm_viterbi/" ><i class="fa fa-file-o"></i>隐马尔可夫模型（三）：Viterbi 算法</a>
      </li>
    
      <li>
        <a href="/2014/10/06/hmm_forward_algorithm/" ><i class="fa fa-file-o"></i>隐马尔可夫模型（二）：前向算法</a>
      </li>
    
      <li>
        <a href="/2014/09/05/hidden_morkov_model/" ><i class="fa fa-file-o"></i>隐马尔可夫模型（一）：介绍</a>
      </li>
    
  </ul>
</div>


		
			<!-- _widget/links -->

<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="http://blog.csdn.net/aivin24" title="Freewill's Github repository." target="_blank"]);">My CSDN Blog</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->




	  </div>
    <footer> <!-- _partial/footer -->
<p>
  &copy; 2014 by Jinwen
  &nbsp;
  <span class="text-muted">Powered by:
    <a class="text-muted" href="http://zespia.tw/hexo/" target="_blank">Hexo</a>,
    <a class="text-muted" href="http://github.com/yieme/hexo-theme-freewill/">Freewill</a>
    &amp;
    <a class="text-muted" href="http://bootswatch.com/" target="_blank">Bootswatch <small>v3.2</small></a>
  </span>
</p>
 </footer>
  </div> <!-- container-narrow -->
  <!-- _partial/after_footer -->
<a id="gotop" href="#">
  <span>▲</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body></html>
