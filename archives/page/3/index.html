<!DOCTYPE HTML><!-- _partial/head -->
<html>
<head>
  <meta charset="utf-8">
  
  <title>第 3 页 | 归档 | 风清阳明</title>
  <meta name="author" content="Jinwen">
  
  <meta name="description" content="互联网，机器学习，金融，生活">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="风清阳明"/>

  
    <meta property="og:image" content="undefined"/>
  

  
    <link rel="alternative" href="/true" title="风清阳明" type="application/atom+xml">
  
  
    <link href="/favicon.png" rel="icon">
  

  
  <link rel="stylesheet" href="/css/bootstrap.cerulean.min.css" media="screen" type="text/css">
  
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  <!-- _partial/post/google_analytics -->



</head>

 <body>
  <!-- _partial/navigation -->
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">风清阳明</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 <!-- archive -->
<!-- _partial/archive -->


<!-- archive.title -->
<div class="page-header">
  <h1 class="archive-title">归档</h1>
</div>

<div class="row page">
  <!-- cols -->
  
  <div class="col-md-9">
	

	  
	  <!-- display as entry -->
	  <div class="mypage">
	  
	  <!-- _partial/post/title -->
<div class="panel-heading">

	
	<!-- display as entry -->
		<h3 class="xtitle">
			<a href="/2014/05/28/logistic-regression/" >Logistic 回归</a>
			<span class="text-muted pull-right"><small> 5月 28 2014 </small></span>
		</h3>
	

</div>

	  <!-- _partial/post/entry -->
<div class="panel-body">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <p>（把公式转化成图片然后再插入页面的做法实在是麻烦，主要是每次都要加一个<img>这种标签，完全丢掉了直接写latex的方便。现在终于可以直接在.md文件中直接插入公式了，^_^）</p>
<h1 id="介绍">介绍</h1>
<p>考虑超平面分类方法，假设目前确定好一个超平面$W^TX = 0$，对于待测数据x，如果$W^Tx &gt; 0$，则$x$属于正例，否则属于负例。这种分类函数就是一个阶跃函数，其边界不好确定。现在采用sigmod函数代替这一分类函数：<br>$$g(z) = \frac{1}{1+e^{-z}}$$<br>其中$z = W^Tx$。</p>
<p>现在的分类方法就是：如果g(z) &gt; 0.5,则x属于正例，否则属于负例。这里完全是用概率来进行分类，如下：<br>$$P(y=1|x,\theta) = g(W^Tx)$$<br>$$P(y=0|x,\theta) = 1 - g(W^Tx)$$</p>
<p>sigmod函数是连续函数，而且其导数具有一个非常重要的特性，如下：<br>$$g’(z) = g(z)(1-g(z))$$</p>
<p>后面会利用到这个特征。</p>
<h1 id="损失函数">损失函数</h1>
<p>logistic 回归的损失函数是通过最大似然估计出来的。现在假设分类函数为$h(x) = g(W^Tx)$，那么对x进行分类的公式就是:<br>$$P(y|x;\theta) = h(x)^y(1 - h(x))^{1-y}$$<br>得到似然函数(为了和似然函数常规写法一致，将用$\theta$代替$W$表示需要学习的参数)<br>$$L(\theta) = \prod_{i=1}^n P(y^i|x^i;\theta)$$  </p>
<p>对数似然函数就是:<br>$$l(\theta) = \log L(\theta) = \sum_{i=1}^n (y^i\log h(x^i) + (1-y^i)\log(1 - h(x^i)))$$</p>
<p>这里是需要最大化似然函数，为了后面最小化损失函数（其实原理一样，只是表述不同罢了），令损失函数为:<br>$$J(\theta) = -\frac{1}{m}l(\theta)$$</p>
<p>到目前为止，损失函数已经得出，实际训练的目的就是通过训练数据$X$找到最优的参数$\theta^{\star}$，使损失函数$J(\theta)$最小。</p>
<h1 id="训练：梯度下降">训练：梯度下降</h1>
<p>如果把损失函数的图像看作是一个碗型的峡谷，不同的参数对应着这块凹地上不同的点。现在，我们的目的是要走到谷底去找小龙女，注意这里只考虑一个谷底。如何又快又准确的去找呢？只要采取两种措施就一定能够走到谷底：1. 永远要往下走；2. 一定要往最陡的方向走。第一种方法保证我们不会南辕北辙，第二种方法保证我们走到目的地的速度最快。</p>
<p>在这里损失函数$J(\theta)$的值通过$\theta$来确定。因为在任何一点，只要朝着梯度方向的相反方向走，那一定满足前面提到的两个条件，即最快得往下走，如下：</p>
<p>$$\theta_j = \theta_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta)$$</p>
<p>其中$\partial即是学习速率$，在这里体现为每一步跨越多大。实际中，需要逐渐减少这一值，因为如果在接近谷底的时候，要“小心谨慎”得走下去，如果跨越太大，有可能会避开谷底，跑到另一个斜面上去了。如果是梯度上升，则将减号换成加号。</p>
<p>梯度的结果是（根据sigmod函数来进行求导计算)：</p>
<p>$$\frac{\partial}{\partial\theta_j}J(\theta)=$$  </p>
<p>$$\frac{1}{m} \sum_{i=1}^m (h(x^i) - y^i)x^i_j$$</p>
<h1 id="实例">实例</h1>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn, classLabels)</span>:</span>
    dataMatrix = mat(dataMatIn)             
    labelMat = mat(classLabels).transpose() 
    m,n = shape(dataMatrix)  <span class="comment"># m,n分别是训练数据的数目和特征数目</span>
    alpha = <span class="number">0.001</span>            <span class="comment"># 学习速率</span>
    maxCycles = <span class="number">500</span>          <span class="comment"># 梯度下降次数</span>
    weights = ones((n,<span class="number">1</span>))     <span class="comment"># 初始化权重，即在坡面任取一点</span>
    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):              
        h = sigmoid(dataMatrix*weights)     <span class="comment">#模型的预测</span>
        error = (h - labelMat)              <span class="comment">#梯度中的差项，见上面公式</span>
        weights = weights - alpha * dataMatrix.transpose()* error <span class="comment"># 参数变化，见上面公式  </span>
    <span class="keyword">return</span> weights
</code></pre><p>在这里学习速率没有发生变化，且一次参数更改中，所有数据都参与了。实际中，训练数据有可能不能一次性装入内存，这时需要依次读入数据，依次更改参数$\theta$，这种方法称之为“在线梯度学习”，且每次选的数据随机选取，这样称为随机梯度下降。而且学习速率需要在每次迭代后减少，体现为最终接近谷底的速度越来越慢，更能准确得到达谷底。</p>
<p>参考<a href="http://www.manning.com/pharrington/" target="_blank">《机器学习实战》</a>。</p>

	
	</div>
	

</div>
	<a type="button" href="/2014/05/28/logistic-regression/#more" class="btn btn-default more">阅读此文</a>
</div>

	  
	  <!-- _partial/post/title -->
<div class="panel-heading">

	
	<!-- display as entry -->
		<h3 class="xtitle">
			<a href="/2014/05/27/bayes/" >朴素贝叶斯分类方法</a>
			<span class="text-muted pull-right"><small> 5月 27 2014 </small></span>
		</h3>
	

</div>

	  <!-- _partial/post/entry -->
<div class="panel-body">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <p>贝叶斯分类的基础是概率论。和kNN、决策树这些分类方法有点不同，基于概率的分类方法是通过最大化概率来实现分类的。这就是说，当前待分类数据可能属于所有类，计算出各自的概率，然后选概率最大的类作为分类结果。贝叶斯分类就是一种基本的概率方法，需要利用到贝叶斯公式。朴素贝叶斯分类的假设前提是条件独立，即在各个种类中，每个特征的出现和其他特征没有关系。独立性很简单，假设现在有两个字:”我”、“们”，在日常用语之中，这两个字并不是完全没有关系的，他们之中的某一个出现了，那一个字的出现几率会增大，所以他们不是完全独立的。条件独立稍微有点变化，只是将范围缩小至一个特定的类别而已，我们只在这个小范围考虑独立性。</p>
<h1 id="介绍">介绍</h1>
<p>假设现在需要判断一封邮件是否是垃圾邮件，根据朴素贝叶斯分类方法的原理，我们需要计算当前邮件属于垃圾邮件和不属于垃圾邮件各自的概率，判断的公式如下：  </p>
<ul>
<li>如果p1（邮件）&gt; p2(邮件)， 则邮件是垃圾邮件；</li>
<li>如果p1（邮件）&lt; p2(邮件)， 则邮件是正常邮件；</li>
</ul>
<p>其中p1和p2分别是垃圾邮件和正常邮件的概率。</p>
<p>为了计算每个类别的概率，即$<br>p(c_i|x) $，需要利用贝叶斯公式：<br>$$<br>p(c_i|x) = \frac{p(x|c_i)p(c_i)}{p(x)}<br>$$<br>通过计算等式右边的三项得到分类概率。</p>
<p>为什么要这样去计算右边三项？右边三项是很容易求出来的，比方说计算p(ci)，只要统计每个类别出现的频率就可以了，其他两项也是一样，不好计算左边项。</p>
<h1 id="实现">实现</h1>
<p>为了便于程序实现朴素贝叶斯分类器，需要经过以下几步：   </p>
<ol>
<li>通过训练数据集构造一个词集，即搜集一些单词作为整个数据集的单词来源；</li>
<li>数据向量化，通过扫描当前数据，如果单词出现在词典中，则相应位置置为1，否则是0。这样向量的长度为词典的大小；</li>
<li>所有训练数据的向量组成一个训练矩阵，相应的类别组成一个类别向量。根据训练矩阵和类别向量计算出贝叶斯公式的右边项的分子项。</li>
<li>将待测数据的向量代入公式的右边，计算出属于每一类的概率。</li>
</ol>
<h2 id="构建词向量">构建词向量</h2>
<p>初始化词典单词的来源</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span>
    <span class="comment">#初始化6个单词列表</span>
    postingList=[[<span class="string">'my'</span>, <span class="string">'dog'</span>, <span class="string">'has'</span>, <span class="string">'flea'</span>, <span class="string">'problems'</span>, <span class="string">'help'</span>, <span class="string">'please'</span>],
                 [<span class="string">'maybe'</span>, <span class="string">'not'</span>, <span class="string">'take'</span>, <span class="string">'him'</span>, <span class="string">'to'</span>, <span class="string">'dog'</span>, <span class="string">'park'</span>, <span class="string">'stupid'</span>],
                 [<span class="string">'my'</span>, <span class="string">'dalmation'</span>, <span class="string">'is'</span>, <span class="string">'so'</span>, <span class="string">'cute'</span>, <span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'him'</span>],
                 [<span class="string">'stop'</span>, <span class="string">'posting'</span>, <span class="string">'stupid'</span>, <span class="string">'worthless'</span>, <span class="string">'garbage'</span>],
                 [<span class="string">'mr'</span>, <span class="string">'licks'</span>, <span class="string">'ate'</span>, <span class="string">'my'</span>, <span class="string">'steak'</span>, <span class="string">'how'</span>, <span class="string">'to'</span>, <span class="string">'stop'</span>, <span class="string">'him'</span>],
                 [<span class="string">'quit'</span>, <span class="string">'buying'</span>, <span class="string">'worthless'</span>, <span class="string">'dog'</span>, <span class="string">'food'</span>, <span class="string">'stupid'</span>]]
    classVec = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]    <span class="comment"># 1表示是垃圾邮件，0则相反</span>
    <span class="keyword">return</span> postingList,classVec 
</code></pre><p>创建词典          </p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span>
    vocabSet = set([])  <span class="comment">#集合中元素不能重复</span>
    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:
        vocabSet = vocabSet | set(document) <span class="comment">#求集合的并</span>
    <span class="keyword">return</span> list(vocabSet)
</code></pre><p>单词向量化</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2Vec</span><span class="params">(vocabList, inputSet)</span>:</span>
    returnVec = [<span class="number">0</span>]*len(vocabList)  <span class="comment">#向量的长度就是词典的大小</span>
    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:
        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:
            returnVec[vocabList.index(word)] = <span class="number">1</span>  <span class="comment"># 如果单词在该词典中出现，则置相应位置为1</span>
        <span class="keyword">else</span>: <span class="keyword">print</span> <span class="string">"the word: %s is not in my Vocabulary!"</span> % word
    <span class="keyword">return</span> returnVec    <span class="comment">#注意这里返回一个0-1向量矩阵，和词袋模型（后面会提到）有区别</span>
</code></pre><h2 id="训练模型：计算概率">训练模型：计算概率</h2>
<p>根据训练矩阵和类别向量可以计算p(x|c)和p(c)，但是由于条件独立性的假设，即:<br>$$<br>p(x|c_i) = p(x_1|c_i)p(x_2|c_i)\ldots p(x_n|c_i)<br>$$<br>如果某一项特征为0，那么最后的值都为0，所以需要将每一项初始化为1，分母设置为2。此外，因为每一项的概率值非常小，如果多项相乘则会产生下溢出，所以需要将其对数化，转成求和。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix,trainCategory)</span>:</span>
    numTrainDocs = len(trainMatrix)   
    numWords = len(trainMatrix[<span class="number">0</span>])
    pAbusive = sum(trainCategory)/float(numTrainDocs)   <span class="comment">#即模型中的p(ci)</span>
    p0Num = ones(numWords); p1Num = ones(numWords)   <span class="comment">#分子分母分别初始化为1、2   </span>
    p0Denom = <span class="number">2.0</span>; p1Denom = <span class="number">2.0</span>                        
    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):
        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:
            p1Num += trainMatrix[i]  <span class="comment">#将每一项的特征累加</span>
            p1Denom += sum(trainMatrix[i]) <span class="comment">#所有特征累加</span>
        <span class="keyword">else</span>:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    p1Vect = log(p1Num/p1Denom)          <span class="comment"># 每一项特征出现的概率，对应于p(x_1|c),...,p(x_n|c)</span>
    p0Vect = log(p0Num/p0Denom)          
    <span class="keyword">return</span> p0Vect,p1Vect,pAbusive
</code></pre><h1 id="测试算法">测试算法</h1>
<p>结合训练过程得到的几个参数p(c)和p(x|c)，将测试数据的特征向量（0-1向量）转化成p(x|c)，最后得到每一类的条件概率p（c|x）。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span>
    p1 = sum(vec2Classify * p1Vec) + log(pClass1)     <span class="comment">#转化为对数求和</span>
    p0 = sum(vec2Classify * p0Vec) + log(<span class="number">1.0</span> - pClass1)
    <span class="keyword">if</span> p1 &gt; p0:
        <span class="keyword">return</span> <span class="number">1</span>
    <span class="keyword">else</span>: 
        <span class="keyword">return</span> <span class="number">0</span>
</code></pre><h1 id="词袋模型">词袋模型</h1>
<p>前面的“词集”只考虑了每个特征单词是否出现，没有记录次数，词袋模型则是考虑了每一个训练特征出现的次数，所以，向量的生成过程变成如下形式：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span><span class="params">(vocabList, inputSet)</span>:</span>
    returnVec = [<span class="number">0</span>]*len(vocabList)
    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:
        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:
            returnVec[vocabList.index(word)] += <span class="number">1</span>  <span class="comment">#累计单词的次数</span>
    <span class="keyword">return</span> returnVec
</code></pre>
	
	</div>
	

</div>
	<a type="button" href="/2014/05/27/bayes/#more" class="btn btn-default more">阅读此文</a>
</div>

	  
	  <!-- _partial/post/title -->
<div class="panel-heading">

	
	<!-- display as entry -->
		<h3 class="xtitle">
			<a href="/2014/05/25/desicison_tree/" >决策树</a>
			<span class="text-muted pull-right"><small> 5月 25 2014 </small></span>
		</h3>
	

</div>

	  <!-- _partial/post/entry -->
<div class="panel-body">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <p>每个数据都存有若干的特征值，这些特征值组成一个特征向量。但是，特征之间的等级关系是不一样的，就是说每个特征对于最终的类别影响是不一样的。比如说现在根据两种特征来判定某动物是不是属于鸟类，这两种特征分别是：是否能飞，是否有两只脚。因为鸟类都有翅膀，但不是只有鸟类才有翅膀，所以，根据我们的直觉，应该先看该动物是否有翅膀来进行判断。在这里，“飞”比“两只脚”的特征等级要高。</p>
<h1 id="介绍">介绍</h1>
<p>决策树就是一个根据特征的优先关系来进行分类的，假设目前的决策树已经形成，那么分类的时候就是依次从根节点遍历到叶子节点，每个节点代表一种特征，深度越小的节点优先级越高。</p>
<p>那么如何构造出相应的决策树呢？  </p>
<ol>
<li>我们要把训练数据进行合适的划分，这一步需要选出最好的特征（等级最高，后面介绍如何评价特征的好坏或等级）；  </li>
<li>以选出的特征作为节点，形成子树（有多少个特征值，就有多少颗子树）。如果所有的数据都是同一类，那么终止划分，否则，执行第3步骤；</li>
<li>在每个子树数据集中执行第1步骤。</li>
</ol>
<p>这是一个递归过程，反复执行构造步骤，直到数据都属于同一类。</p>
<h1 id="划分数据集">划分数据集</h1>
<p>选择一个最好的特征，根据这个特征值，将数据分成若干子集，这是划分数据集的过程。如何衡量一个特征是不是最好？需要利用信息论里面“熵”，这是量化信息的一个基本概念，衡量信息的期望值。</p>
<h2 id="计算熵">计算熵</h2>
<p>假设一共有n类，那么熵就是：<br>$$<br>H = -\sum_{i=1}^np(x_i)log_2p(x_i)<br>$$</p>
<p>代码如下：  </p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span>
    numEntries = len(dataSet)  <span class="comment"># 计算数据集的数量</span>
    labelCounts = {}           <span class="comment">#每个类别的数量</span>
    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet: 
        currentLabel = featVec[-<span class="number">1</span>]    <span class="comment"># 最后一列存的是类别</span>
        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys(): <span class="comment"># 第一次找到该类别</span>
            labelCounts[currentLabel] = <span class="number">0</span>
        labelCounts[currentLabel] += <span class="number">1</span>
    shannonEnt = <span class="number">0.0</span>
    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:
        prob = float(labelCounts[key])/numEntries   <span class="comment"># 用发生的频率近似概率</span>
        shannonEnt -= prob * log(prob,<span class="number">2</span>) <span class="comment">#log base 2  #熵的每一项</span>
    <span class="keyword">return</span> shannonEnt
</code></pre><h2 id="按给定特征划分数据集">按给定特征划分数据集</h2>
<p>当得出最优特征时，根据每个数据在当前特征的取值划分，取值相同的分为同一组，比如根据性别来分时，性别有两种取值，那么将当前数据集分成两组即可，所有男的分成一组，所有女的分成一组。</p>
<p>代码如下：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span>
    <span class="comment"># axis 是最优特征，value是特征值</span>
    retDataSet = []   <span class="comment"># 新建一个数据列表</span>
    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:
        <span class="keyword">if</span> featVec[axis] == value:
            reducedFeatVec = featVec[:axis]     
            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])
            retDataSet.append(reducedFeatVec)
    <span class="keyword">return</span> retDataSet
</code></pre><p>以上代码注意append和extend的差别，同一个列表功能一样，但是多个列表时，extend的元素会形成列表，append形成的元素和原先的列表元素相同。</p>
<h2 id="选择最优特征">选择最优特征</h2>
<p>最优特征的选取需要用到前面介绍的熵（信息期望）。熵是用来衡量信息多少的，即量化信息。熵越大，说明数据越乱，熵越小，说明数据越有序。最优特征需要保证划分后的数据比其他特征划分的数据更有序。因为有序程度可以通过熵来表示，所以实际中通过当前熵与划分熵的差来表示前后数据的变化，即信息增益。</p>
<p>代码如下：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span>
    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span>   <span class="comment"># 特征数要去掉最后的类别项</span>
    baseEntropy = calcShannonEnt(dataSet)  <span class="comment"># 数据集划分之前的熵</span>
    bestInfoGain = <span class="number">0.0</span>; bestFeature = -<span class="number">1</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):       <span class="comment"># 遍历所有特征以便选取最优特征</span>
        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]    <span class="comment">#抽取所有数据中的第i个特征 </span>
        uniqueVals = set(featList)     <span class="comment"># 集合中每一个元素都是唯一的，所以每个特征值也是唯一的</span>
        newEntropy = <span class="number">0.0</span>    
        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)  <span class="comment">#根据第i个特征的值value划分数据</span>
            prob = len(subDataSet)/float(len(dataSet)) <span class="comment"># 当前子集占所有集合的频率</span>
            newEntropy += prob * calcShannonEnt(subDataSet)  <span class="comment">#划分后数据集的熵</span>
        infoGain = baseEntropy - newEntropy     <span class="comment">#信息增益即是熵的减少</span>
        <span class="comment">#根据最大的信息增益选出最优特征</span>
        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):       
            bestInfoGain = infoGain          
            bestFeature = i
    <span class="keyword">return</span> bestFeature      
</code></pre><h1 id="构造决策树">构造决策树</h1>
<p>构造决策树是一个递归过程，只要当前数据集中的实例不是同一类就需要继续划分。当然，有的时候特征已经选取完了，但是数据集中还是有实例不是同一类。这个时候就需要投票表决，所占实例的数量最多的一项作为该叶子节点的类别。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span>
    classCount={}
    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:
        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys(): classCount[vote] = <span class="number">0</span>
        classCount[vote] += <span class="number">1</span>
    <span class="comment">#根据数量进行排序</span>
    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)  
    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]
</code></pre><p>创建决策树有两种情况终止：  </p>
<ol>
<li>所有特征已经选取完；  </li>
<li>所有实例属于同一类；</li>
</ol>
<p>代码如下：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,labels)</span>:</span>
    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]
    <span class="comment">#第一种终止情况</span>
    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList): 
        <span class="keyword">return</span> classList[<span class="number">0</span>]
    <span class="comment">#第二种终止情况</span>
    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>: 
        <span class="keyword">return</span> majorityCnt(classList)
    bestFeat = chooseBestFeatureToSplit(dataSet)
    bestFeatLabel = labels[bestFeat]
    myTree = {bestFeatLabel:{}}
    <span class="keyword">del</span>(labels[bestFeat])  
    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]
    uniqueVals = set(featValues)
    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:
        subLabels = labels[:]        
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
    <span class="keyword">return</span> myTree 
</code></pre><p>参考自<a href="http://www.manning.com/pharrington/" target="_blank">机器学习实战</a>第三章。</p>

	
	</div>
	

</div>
	<a type="button" href="/2014/05/25/desicison_tree/#more" class="btn btn-default more">阅读此文</a>
</div>

	  
	  <!-- _partial/post/title -->
<div class="panel-heading">

	
	<!-- display as entry -->
		<h3 class="xtitle">
			<a href="/2014/05/23/machinelearning_knn/" >k近邻算法</a>
			<span class="text-muted pull-right"><small> 5月 23 2014 </small></span>
		</h3>
	

</div>

	  <!-- _partial/post/entry -->
<div class="panel-body">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <p>这是一篇纯水文，只是想练习一下在博客中插入图片、公式、代码的效果。不过，如果有不知道kNN算法的童鞋，希望这篇博文能够帮助到你。</p>
<h1 id="介绍">介绍</h1>
<p>k近邻算法(kNN)算是机器学习里面最基本的分类算法了，基本思想就是近似的一些物体很有可能属于同一类。有句俗语说得好：“物以类聚，人以群分”，我们为了初步判断一个人是怎样的一个人，我们可以先看看他（她）交的朋友。 假设这个世界就只有三种类型的青年：普通青年、文艺青年和2B青年，对于我们认识的人，我们都知道他属于哪一类型的青年。现在，出现了一位新的青年Z，我们不认识他，那么如何判定他属于哪一种类型的青年呢？</p>
<p>这是机器学习的一个主要研究领域：分类。</p>
<p>对于kNN来说，其大致是这样解决这个问题的：首先我们根据我们认识的人得到每一种类型的人的一些特征。然后看Z和哪些人相像（通过比较他和已知青年的特征做比较，相同特征越多，二者最相像），最后我们根据和Z相像的人的类型大致判定Z的类型。 那么选多少人做比较呢？在kNN中，k就表示选k个人，根据实际应用而加以改变。</p>
<p>那么kNN就是这样的一个流程：   </p>
<ol>
<li>首先有个数据集，都是带属性特征和已知标签的（即知道他们的表现特征、属于哪一类型人）；  </li>
<li>然后根据新数据的特征，计算其和已知数据集的特征距离（多种衡量标准），找出最像的前k个数据（距离越小越相像）；</li>
<li>挑出前k个数据中数量最多的那一种类型，比方说选出的5个人中，有3位文艺青年、1位普通青年、1位2B青年，那么我们采用一种投票的性质，判定新数据表现为文艺青年。</li>
</ol>
<h1 id="模型">模型</h1>
<p>我们称已知类型的数据集为训练集X，标签为Y，新出现的数据是a。注意：X是一个向量集，a是一个向量，y看作是一个向量。那么，我们要得到a的类型pred(a)，根据kNN的思想，模型公式如下：</p>
<p>$$<br>pred(a) = argmax_i[number(i),i=1,\ldots,K]<br>$$<br>其中k表示所有种类的数目，number(i)表示第i种类型在前k个数据中所占的数目。挑选前k个数据是根据他们的特征距离来获得的。</p>
<p>训练数据都是已知标签的，如下图的A、B,特征向量的维度为2。</p>
<p><img src="https://s3-us-west-2.amazonaws.com/droplr.storage/files/acc_263367/JOzd?AWSAccessKeyId=AKIAJSVQN3Z4K7MT5U2A&amp;Expires=1400828892&amp;Signature=QJ%2F7AzfQrOrbt3LqEFj34kiFc%2FE%3D" alt=""></p>
<h1 id="实例">实例</h1>
<p>现在通过python实现kNN算法，详细介绍参见<a href="http://www.manning.com/pharrington/" target="_blank">机器学习实战</a>第二章。</p>
<p>引入模块:   </p>
<pre><code>&gt;&gt;&gt;<span class="keyword">import</span> kNN
</code></pre><p>创建带有属性特征和标签的数据，定义一个相关函数：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span>
    group = array([[<span class="number">1.0</span>,<span class="number">1.1</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]])
    labels = [<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>]
    <span class="keyword">return</span> group, labels
</code></pre><p>定义分类函数：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span>
    dataSetSize = dataSet.shape[<span class="number">0</span>]  <span class="comment">#得出训练数据的个数</span>
    diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) - dataSet <span class="comment"># 计算每个训练数据和待测试数据的差</span>
    sqDiffMat = diffMat**<span class="number">2</span> <span class="comment"># 距离平方</span>
    sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>) <span class="comment">#平方和</span>
    distances = sqDistances**<span class="number">0.5</span> <span class="comment"># 欧几里得距离</span>
    sortedDistIndicies = distances.argsort()   <span class="comment">#距离矩阵排序   </span>
    classCount={}   
    <span class="comment"># 选出距离前k小的数据       </span>
    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):
        voteIlabel = labels[sortedDistIndicies[i]]   <span class="comment">#记录第i小的数据标签，参与后面的投票。</span>
        classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>) + <span class="number">1</span>
    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)
    <span class="comment">#根据投票结果，得出票数最大的标签作为分类结果</span>
<span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]
</code></pre>
	
	</div>
	

</div>
	<a type="button" href="/2014/05/23/machinelearning_knn/#more" class="btn btn-default more">阅读此文</a>
</div>

	  
	  <!-- _partial/post/title -->
<div class="panel-heading">

	
	<!-- display as entry -->
		<h3 class="xtitle">
			<a href="/2014/05/22/my-new-post-1/" >博客前言</a>
			<span class="text-muted pull-right"><small> 5月 22 2014 </small></span>
		</h3>
	

</div>

	  <!-- _partial/post/entry -->
<div class="panel-body">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <p>开启本博客一个最主要的原因是自己想尝试精确简明地表述自己的观点，没有废话。以前也在一些门户网站上面开过一些博客，但是一直没有坚持下来，主要是两个方面的原因：</p>
<ol>
<li>自己水平不够，总感觉写的东西连自己都看不下去，总是写了又删，删了又写，特没意思；</li>
<li>没有养成写博客做记录的习惯，没有一直坚持下来。</li>
</ol>
<p>不过，总觉得自己应该时不时总结下自己的观点，准确清晰的表达能力算是一门很重要的技能，以前，自己一直这么认为：自己知道的没必要和别人讨论！自己不知道的，还和别人讨论什么？还不如自己好好去学习。</p>
<p>这一次，果断申请了一个域名，建立了一个独立博客，再一次拾起这份热情。因为本人实在是水平太一般，所以只希望能够记录下自己平时的学习总结，对生活体会的观点，看到自己前进。</p>
<p>本博客的主题定位：<br>热爱互联网，属于人工智能，关注金融，享受生活！</p>

	
	</div>
	

</div>
	<a type="button" href="/2014/05/22/my-new-post-1/#more" class="btn btn-default more">阅读此文</a>
</div>

	  
	  </div>
	  <div>
	  <center>
	  <!-- _partial/index_pagination -->
<div class="pagination">
<ul class="pagination">
	 
		
    	<li class="prev"><a href="/archives/page/2/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i> 上一页</a></li>
  		

        <li><a href="/"><i class="fa fa-home"></i>Home</a></li>

		
          <li class="next disabled"><a>下一页<i class="fa fa-arrow-circle-o-right"></i></a></li>
        
	
</ul>
</div>

	  </center>
	  </div>

	  

</div> <!-- col-md-9/col-md-12 -->


<!-- _partial/sidebar -->
<div class="col-md-3">
	<div id="sidebar">
	
			<!-- _widget/search -->
<div class="form-group has-success has-feedback">
  <form action="//google.com/search" method="get" accept-charset="utf-8" >
    <input type="search" name="q" results="0" placeholder="搜索" class="form-control">
    <input type="hidden" name="q" value="site:xiaojinwen.com">
  </form>
</div>

		
			<!-- _widget/category -->

	<div class="widget">
		<h4>分类</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/Python/">Python<span>1</span></a></li>
		
			<li><a href="/categories/人工智能/">人工智能<span>10</span></a></li>
		
			<li><a href="/categories/工作/">工作<span>1</span></a></li>
		
			<li><a href="/categories/机器学习/">机器学习<span>1</span></a></li>
		
			<li><a href="/categories/生活/">生活<span>1</span></a></li>
		
			<li><a href="/categories/读书/">读书<span>1</span></a></li>
		
		</ul>
	</div>


		
			<!-- _widget/tagcloud -->

	<div class="widget">
		<h4>标签云</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/tags/机器学习实战/">机器学习实战<span>7</span></a></li>
		
			<li><a href="/tags/王小波/">王小波<span>1</span></a></li>
		
			<li><a href="/tags/说明/">说明<span>1</span></a></li>
		
			<li><a href="/tags/HMM/">HMM<span>3</span></a></li>
		
			<li><a href="/tags/人工智能/">人工智能<span>1</span></a></li>
		
			<li><a href="/tags/Socket/">Socket<span>1</span></a></li>
		
			<li><a href="/tags/求职/">求职<span>1</span></a></li>
		
		
		</ul>
	</div>


		
			<!-- _widget/recent_posts -->

<div class="widget">
  <h4>最新文章</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2014/12/14/job_finding/" ><i class="fa fa-file-o"></i>求职小结</a>
      </li>
    
      <li>
        <a href="/2014/11/13/silent_majority/" ><i class="fa fa-file-o"></i>沉默的大多数</a>
      </li>
    
      <li>
        <a href="/2014/10/07/hmm_viterbi/" ><i class="fa fa-file-o"></i>隐马尔可夫模型（三）：Viterbi 算法</a>
      </li>
    
      <li>
        <a href="/2014/10/06/hmm_forward_algorithm/" ><i class="fa fa-file-o"></i>隐马尔可夫模型（二）：前向算法</a>
      </li>
    
      <li>
        <a href="/2014/09/05/hidden_morkov_model/" ><i class="fa fa-file-o"></i>隐马尔可夫模型（一）：介绍</a>
      </li>
    
  </ul>
</div>


		
			<!-- _widget/links -->

<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="http://blog.csdn.net/aivin24" title="Freewill's Github repository." target="_blank"]);">My CSDN Blog</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->




	  </div>
    <footer> <!-- _partial/footer -->
<p>
  &copy; 2014 by Jinwen
  &nbsp;
  <span class="text-muted">Powered by:
    <a class="text-muted" href="http://zespia.tw/hexo/" target="_blank">Hexo</a>,
    <a class="text-muted" href="http://github.com/yieme/hexo-theme-freewill/">Freewill</a>
    &amp;
    <a class="text-muted" href="http://bootswatch.com/" target="_blank">Bootswatch <small>v3.2</small></a>
  </span>
</p>
 </footer>
  </div> <!-- container-narrow -->
  <!-- _partial/after_footer -->
<a id="gotop" href="#">
  <span>▲</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body></html>
